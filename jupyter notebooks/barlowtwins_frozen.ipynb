{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6bcqDGPelrF",
        "outputId": "f96b7cee-10e6-4dcf-87ff-791c3c0c4618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMGowi4LfhAV",
        "outputId": "f5a761b7-0883-4548-f661-d5eba532c97f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution time: 4 minutes\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "!mkdir cityscapes\n",
        "!unzip -qo /content/drive/MyDrive/datasets/leftImg8bit_trainvaltest.zip -d /content/cityscapes\n",
        "!unzip -qo /content/drive/MyDrive/datasets/gtFine_trainvaltest.zip -d /content/cityscapes\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "execution_time = int(execution_time/60)\n",
        "print(\"Execution time:\", execution_time, \"minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3ilxErqiGNF",
        "outputId": "045bfefc-9bbd-424f-bbf7-96aff22b824a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.4 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.4 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.4 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.6/473.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q visdom ood_metrics cityscapesscripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9524JHa5P3AI",
        "outputId": "946b7b60-31a9-4f21-ef49-3ed983b24b48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cityscapesScripts'...\n",
            "remote: Enumerating objects: 648, done.\u001b[K\n",
            "remote: Counting objects: 100% (221/221), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 648 (delta 184), reused 165 (delta 155), pack-reused 427\u001b[K\n",
            "Receiving objects: 100% (648/648), 796.28 KiB | 2.45 MiB/s, done.\n",
            "Resolving deltas: 100% (370/370), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/mcordts/cityscapesScripts.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk-D7IPLiJJF",
        "outputId": "16a354cb-c96e-4f73-c8d1-f95669297b3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 5000 annotation files\n",
            "Progress: 100.0 % Execution time: 2 minutes\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "import os\n",
        "os.environ[\"CITYSCAPES_DATASET\"] = \"/content/cityscapes\"\n",
        "!python cityscapesScripts/cityscapesscripts/preparation/createTrainIdLabelImgs.py\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "execution_time = int(execution_time/60)\n",
        "print(\"Execution time:\", execution_time, \"minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sncgFrG3XQ-I",
        "outputId": "343c9ba7-0019-437f-a16a-95b241422fea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'anomaly-segmentation-for-road-scenes'...\n",
            "remote: Enumerating objects: 763, done.\u001b[K\n",
            "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 763 (delta 41), reused 66 (delta 26), pack-reused 681\u001b[K\n",
            "Receiving objects: 100% (763/763), 562.14 MiB | 40.52 MiB/s, done.\n",
            "Resolving deltas: 100% (159/159), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/shayanit/anomaly-segmentation-for-road-scenes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TA9TU1JM6e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "985df2a8-4de9-4443-d849-5e90d53583c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/anomaly-segmentation-for-road-scenes/train\n"
          ]
        }
      ],
      "source": [
        "%cd /content/anomaly-segmentation-for-road-scenes/train\n",
        "from main import MyCoTransformExtension\n",
        "from dataset import cityscapes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "barlowtwins_resnet = torch.hub.load('facebookresearch/barlowtwins:main', 'resnet50')\n",
        "for param in barlowtwins_resnet.parameters():\n",
        "    param.requires_grad = False #FREEZE\n",
        "barlowtwins_resnet = nn.Sequential(*list(barlowtwins_resnet.children())[:-2])\n",
        "num_classes = 20\n",
        "segmentation_head = nn.Conv2d(2048, num_classes, kernel_size=1)\n",
        "upsample = nn.Upsample(size=(224,224), mode='bilinear', align_corners=False)\n",
        "BarlowTwinsModel = nn.Sequential(barlowtwins_resnet, segmentation_head, upsample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfrbgNDGZpyr",
        "outputId": "28c9bcc3-c007-4cc4-82f1-31cdec0a2a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/barlowtwins/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/barlowtwins/ep1000_bs2048_lrw0.2_lrb0.0048_lambd0.0051/resnet50.pth\" to /root/.cache/torch/hub/checkpoints/resnet50.pth\n",
            "100%|██████████| 90.0M/90.0M [00:00<00:00, 101MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5F6Dqm_gNKh",
        "outputId": "6670692e-c7c4-4f2f-faac-5707b4ebae5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cityscapes/leftImg8bit/train\n",
            "/content/cityscapes/leftImg8bit/val\n",
            "torch.Size([3, 224, 224])\n",
            "torch.Size([1, 224, 224])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.hub import load as hub_load\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Load Cityscapes dataset\n",
        "datadir = '/content/cityscapes'\n",
        "co_transform = MyCoTransformExtension(False, augment=True)\n",
        "co_transform_val = MyCoTransformExtension(False, augment=False)\n",
        "dataset_train = cityscapes(datadir, co_transform, 'train')\n",
        "dataset_val = cityscapes(datadir, co_transform_val, 'val')\n",
        "\n",
        "loader = DataLoader(dataset_train, num_workers=2, batch_size=8, shuffle=True)\n",
        "loader_val = DataLoader(dataset_val, num_workers=2, batch_size=8, shuffle=False)\n",
        "\n",
        "print(dataset_train[0][0].shape)\n",
        "print(dataset_train[0][1].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHN0-JeEghRe"
      },
      "source": [
        "#Train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FYT3KC7fGtu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84d3ee5a-f73f-4862-fdcd-63d5edf192aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, batch 10/372\n",
            "Epoch 1, batch 20/372\n",
            "Epoch 1, batch 30/372\n",
            "Epoch 1, batch 40/372\n",
            "Epoch 1, batch 50/372\n",
            "Epoch 1, batch 60/372\n",
            "Epoch 1, batch 70/372\n",
            "Epoch 1, batch 80/372\n",
            "Epoch 1, batch 90/372\n",
            "Epoch 1, batch 100/372\n",
            "Epoch 1, batch 110/372\n",
            "Epoch 1, batch 120/372\n",
            "Epoch 1, batch 130/372\n",
            "Epoch 1, batch 140/372\n",
            "Epoch 1, batch 150/372\n",
            "Epoch 1, batch 160/372\n",
            "Epoch 1, batch 170/372\n",
            "Epoch 1, batch 180/372\n",
            "Epoch 1, batch 190/372\n",
            "Epoch 1, batch 200/372\n",
            "Epoch 1, batch 210/372\n",
            "Epoch 1, batch 220/372\n",
            "Epoch 1, batch 230/372\n",
            "Epoch 1, batch 240/372\n",
            "Epoch 1, batch 250/372\n",
            "Epoch 1, batch 260/372\n",
            "Epoch 1, batch 270/372\n",
            "Epoch 1, batch 280/372\n",
            "Epoch 1, batch 290/372\n",
            "Epoch 1, batch 300/372\n",
            "Epoch 1, batch 310/372\n",
            "Epoch 1, batch 320/372\n",
            "Epoch 1, batch 330/372\n",
            "Epoch 1, batch 340/372\n",
            "Epoch 1, batch 350/372\n",
            "Epoch 1, batch 360/372\n",
            "Epoch 1, batch 370/372\n",
            "Epoch 1, batch 372/372\n",
            "Epoch 1, Training Loss: 2.040247703431755\n",
            "Validation Loss: 1.679991152551439\n",
            "Best model is: 0\n",
            "Epoch 2, batch 10/372\n",
            "Epoch 2, batch 20/372\n",
            "Epoch 2, batch 30/372\n",
            "Epoch 2, batch 40/372\n",
            "Epoch 2, batch 50/372\n",
            "Epoch 2, batch 60/372\n",
            "Epoch 2, batch 70/372\n",
            "Epoch 2, batch 80/372\n",
            "Epoch 2, batch 90/372\n",
            "Epoch 2, batch 100/372\n",
            "Epoch 2, batch 110/372\n",
            "Epoch 2, batch 120/372\n",
            "Epoch 2, batch 130/372\n",
            "Epoch 2, batch 140/372\n",
            "Epoch 2, batch 150/372\n",
            "Epoch 2, batch 160/372\n",
            "Epoch 2, batch 170/372\n",
            "Epoch 2, batch 180/372\n",
            "Epoch 2, batch 190/372\n",
            "Epoch 2, batch 200/372\n",
            "Epoch 2, batch 210/372\n",
            "Epoch 2, batch 220/372\n",
            "Epoch 2, batch 230/372\n",
            "Epoch 2, batch 240/372\n",
            "Epoch 2, batch 250/372\n",
            "Epoch 2, batch 260/372\n",
            "Epoch 2, batch 270/372\n",
            "Epoch 2, batch 280/372\n",
            "Epoch 2, batch 290/372\n",
            "Epoch 2, batch 300/372\n",
            "Epoch 2, batch 310/372\n",
            "Epoch 2, batch 320/372\n",
            "Epoch 2, batch 330/372\n",
            "Epoch 2, batch 340/372\n",
            "Epoch 2, batch 350/372\n",
            "Epoch 2, batch 360/372\n",
            "Epoch 2, batch 370/372\n",
            "Epoch 2, batch 372/372\n",
            "Epoch 2, Training Loss: 1.5468902517390508\n",
            "Validation Loss: 1.455788531000652\n",
            "Best model is: 1\n",
            "Epoch 3, batch 10/372\n",
            "Epoch 3, batch 20/372\n",
            "Epoch 3, batch 30/372\n",
            "Epoch 3, batch 40/372\n",
            "Epoch 3, batch 50/372\n",
            "Epoch 3, batch 60/372\n",
            "Epoch 3, batch 70/372\n",
            "Epoch 3, batch 80/372\n",
            "Epoch 3, batch 90/372\n",
            "Epoch 3, batch 100/372\n",
            "Epoch 3, batch 110/372\n",
            "Epoch 3, batch 120/372\n",
            "Epoch 3, batch 130/372\n",
            "Epoch 3, batch 140/372\n",
            "Epoch 3, batch 150/372\n",
            "Epoch 3, batch 160/372\n",
            "Epoch 3, batch 170/372\n",
            "Epoch 3, batch 180/372\n",
            "Epoch 3, batch 190/372\n",
            "Epoch 3, batch 200/372\n",
            "Epoch 3, batch 210/372\n",
            "Epoch 3, batch 220/372\n",
            "Epoch 3, batch 230/372\n",
            "Epoch 3, batch 240/372\n",
            "Epoch 3, batch 250/372\n",
            "Epoch 3, batch 260/372\n",
            "Epoch 3, batch 270/372\n",
            "Epoch 3, batch 280/372\n",
            "Epoch 3, batch 290/372\n",
            "Epoch 3, batch 300/372\n",
            "Epoch 3, batch 310/372\n",
            "Epoch 3, batch 320/372\n",
            "Epoch 3, batch 330/372\n",
            "Epoch 3, batch 340/372\n",
            "Epoch 3, batch 350/372\n",
            "Epoch 3, batch 360/372\n",
            "Epoch 3, batch 370/372\n",
            "Epoch 3, batch 372/372\n",
            "Epoch 3, Training Loss: 1.404874897772266\n",
            "Validation Loss: 1.367206160984342\n",
            "Best model is: 2\n",
            "Epoch 4, batch 10/372\n",
            "Epoch 4, batch 20/372\n",
            "Epoch 4, batch 30/372\n",
            "Epoch 4, batch 40/372\n",
            "Epoch 4, batch 50/372\n",
            "Epoch 4, batch 60/372\n",
            "Epoch 4, batch 70/372\n",
            "Epoch 4, batch 80/372\n",
            "Epoch 4, batch 90/372\n",
            "Epoch 4, batch 100/372\n",
            "Epoch 4, batch 110/372\n",
            "Epoch 4, batch 120/372\n",
            "Epoch 4, batch 130/372\n",
            "Epoch 4, batch 140/372\n",
            "Epoch 4, batch 150/372\n",
            "Epoch 4, batch 160/372\n",
            "Epoch 4, batch 170/372\n",
            "Epoch 4, batch 180/372\n",
            "Epoch 4, batch 190/372\n",
            "Epoch 4, batch 200/372\n",
            "Epoch 4, batch 210/372\n",
            "Epoch 4, batch 220/372\n",
            "Epoch 4, batch 230/372\n",
            "Epoch 4, batch 240/372\n",
            "Epoch 4, batch 250/372\n",
            "Epoch 4, batch 260/372\n",
            "Epoch 4, batch 270/372\n",
            "Epoch 4, batch 280/372\n",
            "Epoch 4, batch 290/372\n",
            "Epoch 4, batch 300/372\n",
            "Epoch 4, batch 310/372\n",
            "Epoch 4, batch 320/372\n",
            "Epoch 4, batch 330/372\n",
            "Epoch 4, batch 340/372\n",
            "Epoch 4, batch 350/372\n",
            "Epoch 4, batch 360/372\n",
            "Epoch 4, batch 370/372\n",
            "Epoch 4, batch 372/372\n",
            "Epoch 4, Training Loss: 1.3270885172069713\n",
            "Validation Loss: 1.2975650382420374\n",
            "Best model is: 3\n",
            "Epoch 5, batch 10/372\n",
            "Epoch 5, batch 20/372\n",
            "Epoch 5, batch 30/372\n",
            "Epoch 5, batch 40/372\n",
            "Epoch 5, batch 50/372\n",
            "Epoch 5, batch 60/372\n",
            "Epoch 5, batch 70/372\n",
            "Epoch 5, batch 80/372\n",
            "Epoch 5, batch 90/372\n",
            "Epoch 5, batch 100/372\n",
            "Epoch 5, batch 110/372\n",
            "Epoch 5, batch 120/372\n",
            "Epoch 5, batch 130/372\n",
            "Epoch 5, batch 140/372\n",
            "Epoch 5, batch 150/372\n",
            "Epoch 5, batch 160/372\n",
            "Epoch 5, batch 170/372\n",
            "Epoch 5, batch 180/372\n",
            "Epoch 5, batch 190/372\n",
            "Epoch 5, batch 200/372\n",
            "Epoch 5, batch 210/372\n",
            "Epoch 5, batch 220/372\n",
            "Epoch 5, batch 230/372\n",
            "Epoch 5, batch 240/372\n",
            "Epoch 5, batch 250/372\n",
            "Epoch 5, batch 260/372\n",
            "Epoch 5, batch 270/372\n",
            "Epoch 5, batch 280/372\n",
            "Epoch 5, batch 290/372\n",
            "Epoch 5, batch 300/372\n",
            "Epoch 5, batch 310/372\n",
            "Epoch 5, batch 320/372\n",
            "Epoch 5, batch 330/372\n",
            "Epoch 5, batch 340/372\n",
            "Epoch 5, batch 350/372\n",
            "Epoch 5, batch 360/372\n",
            "Epoch 5, batch 370/372\n",
            "Epoch 5, batch 372/372\n",
            "Epoch 5, Training Loss: 1.2768626664915392\n",
            "Validation Loss: 1.2623853569939023\n",
            "Best model is: 4\n",
            "Epoch 6, batch 10/372\n",
            "Epoch 6, batch 20/372\n",
            "Epoch 6, batch 30/372\n",
            "Epoch 6, batch 40/372\n",
            "Epoch 6, batch 50/372\n",
            "Epoch 6, batch 60/372\n",
            "Epoch 6, batch 70/372\n",
            "Epoch 6, batch 80/372\n",
            "Epoch 6, batch 90/372\n",
            "Epoch 6, batch 100/372\n",
            "Epoch 6, batch 110/372\n",
            "Epoch 6, batch 120/372\n",
            "Epoch 6, batch 130/372\n",
            "Epoch 6, batch 140/372\n",
            "Epoch 6, batch 150/372\n",
            "Epoch 6, batch 160/372\n",
            "Epoch 6, batch 170/372\n",
            "Epoch 6, batch 180/372\n",
            "Epoch 6, batch 190/372\n",
            "Epoch 6, batch 200/372\n",
            "Epoch 6, batch 210/372\n",
            "Epoch 6, batch 220/372\n",
            "Epoch 6, batch 230/372\n",
            "Epoch 6, batch 240/372\n",
            "Epoch 6, batch 250/372\n",
            "Epoch 6, batch 260/372\n",
            "Epoch 6, batch 270/372\n",
            "Epoch 6, batch 280/372\n",
            "Epoch 6, batch 290/372\n",
            "Epoch 6, batch 300/372\n",
            "Epoch 6, batch 310/372\n",
            "Epoch 6, batch 320/372\n",
            "Epoch 6, batch 330/372\n",
            "Epoch 6, batch 340/372\n",
            "Epoch 6, batch 350/372\n",
            "Epoch 6, batch 360/372\n",
            "Epoch 6, batch 370/372\n",
            "Epoch 6, batch 372/372\n",
            "Epoch 6, Training Loss: 1.239267851236046\n",
            "Validation Loss: 1.2311357676036774\n",
            "Best model is: 5\n",
            "Epoch 7, batch 10/372\n",
            "Epoch 7, batch 20/372\n",
            "Epoch 7, batch 30/372\n",
            "Epoch 7, batch 40/372\n",
            "Epoch 7, batch 50/372\n",
            "Epoch 7, batch 60/372\n",
            "Epoch 7, batch 70/372\n",
            "Epoch 7, batch 80/372\n",
            "Epoch 7, batch 90/372\n",
            "Epoch 7, batch 100/372\n",
            "Epoch 7, batch 110/372\n",
            "Epoch 7, batch 120/372\n",
            "Epoch 7, batch 130/372\n",
            "Epoch 7, batch 140/372\n",
            "Epoch 7, batch 150/372\n",
            "Epoch 7, batch 160/372\n",
            "Epoch 7, batch 170/372\n",
            "Epoch 7, batch 180/372\n",
            "Epoch 7, batch 190/372\n",
            "Epoch 7, batch 200/372\n",
            "Epoch 7, batch 210/372\n",
            "Epoch 7, batch 220/372\n",
            "Epoch 7, batch 230/372\n",
            "Epoch 7, batch 240/372\n",
            "Epoch 7, batch 250/372\n",
            "Epoch 7, batch 260/372\n",
            "Epoch 7, batch 270/372\n",
            "Epoch 7, batch 280/372\n",
            "Epoch 7, batch 290/372\n",
            "Epoch 7, batch 300/372\n",
            "Epoch 7, batch 310/372\n",
            "Epoch 7, batch 320/372\n",
            "Epoch 7, batch 330/372\n",
            "Epoch 7, batch 340/372\n",
            "Epoch 7, batch 350/372\n",
            "Epoch 7, batch 360/372\n",
            "Epoch 7, batch 370/372\n",
            "Epoch 7, batch 372/372\n",
            "Epoch 7, Training Loss: 1.2104532869272335\n",
            "Validation Loss: 1.1971103236788796\n",
            "Best model is: 6\n",
            "Epoch 8, batch 10/372\n",
            "Epoch 8, batch 20/372\n",
            "Epoch 8, batch 30/372\n",
            "Epoch 8, batch 40/372\n",
            "Epoch 8, batch 50/372\n",
            "Epoch 8, batch 60/372\n",
            "Epoch 8, batch 70/372\n",
            "Epoch 8, batch 80/372\n",
            "Epoch 8, batch 90/372\n",
            "Epoch 8, batch 100/372\n",
            "Epoch 8, batch 110/372\n",
            "Epoch 8, batch 120/372\n",
            "Epoch 8, batch 130/372\n",
            "Epoch 8, batch 140/372\n",
            "Epoch 8, batch 150/372\n",
            "Epoch 8, batch 160/372\n",
            "Epoch 8, batch 170/372\n",
            "Epoch 8, batch 180/372\n",
            "Epoch 8, batch 190/372\n",
            "Epoch 8, batch 200/372\n",
            "Epoch 8, batch 210/372\n",
            "Epoch 8, batch 220/372\n",
            "Epoch 8, batch 230/372\n",
            "Epoch 8, batch 240/372\n",
            "Epoch 8, batch 250/372\n",
            "Epoch 8, batch 260/372\n",
            "Epoch 8, batch 270/372\n",
            "Epoch 8, batch 280/372\n",
            "Epoch 8, batch 290/372\n",
            "Epoch 8, batch 300/372\n",
            "Epoch 8, batch 310/372\n",
            "Epoch 8, batch 320/372\n",
            "Epoch 8, batch 330/372\n",
            "Epoch 8, batch 340/372\n",
            "Epoch 8, batch 350/372\n",
            "Epoch 8, batch 360/372\n",
            "Epoch 8, batch 370/372\n",
            "Epoch 8, batch 372/372\n",
            "Epoch 8, Training Loss: 1.1897885329941267\n",
            "Validation Loss: 1.1791338286702595\n",
            "Best model is: 7\n",
            "Epoch 9, batch 10/372\n",
            "Epoch 9, batch 20/372\n",
            "Epoch 9, batch 30/372\n",
            "Epoch 9, batch 40/372\n",
            "Epoch 9, batch 50/372\n",
            "Epoch 9, batch 60/372\n",
            "Epoch 9, batch 70/372\n",
            "Epoch 9, batch 80/372\n",
            "Epoch 9, batch 90/372\n",
            "Epoch 9, batch 100/372\n",
            "Epoch 9, batch 110/372\n",
            "Epoch 9, batch 120/372\n",
            "Epoch 9, batch 130/372\n",
            "Epoch 9, batch 140/372\n",
            "Epoch 9, batch 150/372\n",
            "Epoch 9, batch 160/372\n",
            "Epoch 9, batch 170/372\n",
            "Epoch 9, batch 180/372\n",
            "Epoch 9, batch 190/372\n",
            "Epoch 9, batch 200/372\n",
            "Epoch 9, batch 210/372\n",
            "Epoch 9, batch 220/372\n",
            "Epoch 9, batch 230/372\n",
            "Epoch 9, batch 240/372\n",
            "Epoch 9, batch 250/372\n",
            "Epoch 9, batch 260/372\n",
            "Epoch 9, batch 270/372\n",
            "Epoch 9, batch 280/372\n",
            "Epoch 9, batch 290/372\n",
            "Epoch 9, batch 300/372\n",
            "Epoch 9, batch 310/372\n",
            "Epoch 9, batch 320/372\n",
            "Epoch 9, batch 330/372\n",
            "Epoch 9, batch 340/372\n",
            "Epoch 9, batch 350/372\n",
            "Epoch 9, batch 360/372\n",
            "Epoch 9, batch 370/372\n",
            "Epoch 9, batch 372/372\n",
            "Epoch 9, Training Loss: 1.171036351271855\n",
            "Validation Loss: 1.167320602469974\n",
            "Best model is: 8\n",
            "Epoch 10, batch 10/372\n",
            "Epoch 10, batch 20/372\n",
            "Epoch 10, batch 30/372\n",
            "Epoch 10, batch 40/372\n",
            "Epoch 10, batch 50/372\n",
            "Epoch 10, batch 60/372\n",
            "Epoch 10, batch 70/372\n",
            "Epoch 10, batch 80/372\n",
            "Epoch 10, batch 90/372\n",
            "Epoch 10, batch 100/372\n",
            "Epoch 10, batch 110/372\n",
            "Epoch 10, batch 120/372\n",
            "Epoch 10, batch 130/372\n",
            "Epoch 10, batch 140/372\n",
            "Epoch 10, batch 150/372\n",
            "Epoch 10, batch 160/372\n",
            "Epoch 10, batch 170/372\n",
            "Epoch 10, batch 180/372\n",
            "Epoch 10, batch 190/372\n",
            "Epoch 10, batch 200/372\n",
            "Epoch 10, batch 210/372\n",
            "Epoch 10, batch 220/372\n",
            "Epoch 10, batch 230/372\n",
            "Epoch 10, batch 240/372\n",
            "Epoch 10, batch 250/372\n",
            "Epoch 10, batch 260/372\n",
            "Epoch 10, batch 270/372\n",
            "Epoch 10, batch 280/372\n",
            "Epoch 10, batch 290/372\n",
            "Epoch 10, batch 300/372\n",
            "Epoch 10, batch 310/372\n",
            "Epoch 10, batch 320/372\n",
            "Epoch 10, batch 330/372\n",
            "Epoch 10, batch 340/372\n",
            "Epoch 10, batch 350/372\n",
            "Epoch 10, batch 360/372\n",
            "Epoch 10, batch 370/372\n",
            "Epoch 10, batch 372/372\n",
            "Epoch 10, Training Loss: 1.1557717084564187\n",
            "Validation Loss: 1.1553831157230197\n",
            "Best model is: 9\n",
            "Execution time: 137 minutes\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# Instantiate the model\n",
        "model = BarlowTwinsModel\n",
        "\n",
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Choose optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Training loop\n",
        "num_epochs=10\n",
        "print_every=10\n",
        "best_model = 0\n",
        "best_val_loss_avg=float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    total_batches = len(loader)\n",
        "    for batch_idx, (images, labels) in enumerate(loader):\n",
        "        if (batch_idx + 1) % print_every == 0 or (batch_idx + 1) == total_batches:\n",
        "          print(f\"Epoch {epoch+1}, batch {batch_idx+1}/{total_batches}\")\n",
        "        optimizer.zero_grad()\n",
        "        labels = labels.squeeze(1)\n",
        "        # print(f\"images shape: {images.shape}\")\n",
        "        outputs = model(images)\n",
        "        # print(f\"outputs shape: {outputs.shape}\")\n",
        "        labels = labels.long()\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Training Loss: {running_loss / len(loader)}\")\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_running_loss = 0.0\n",
        "    val_total_batches = len(loader_val)\n",
        "\n",
        "    with torch.no_grad():  # Turn off gradient calculation during validation\n",
        "        for batch_idx, (val_images, val_labels) in enumerate(loader_val):\n",
        "            val_labels = val_labels.squeeze(1)\n",
        "            val_outputs = model(val_images)\n",
        "            val_labels = val_labels.long()\n",
        "            val_loss = criterion(val_outputs, val_labels)\n",
        "            val_running_loss += val_loss.item()\n",
        "\n",
        "    val_loss_avg = val_running_loss / len(loader_val)\n",
        "    print(f\"Validation Loss: {val_loss_avg}\")\n",
        "    torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': criterion,\n",
        "                }, f\"/content/drive/MyDrive/datasets/barlowtwins-frozen-{epoch+1}.pth\")\n",
        "    if (val_loss_avg<best_val_loss_avg):\n",
        "      best_val_loss_avg=val_loss_avg\n",
        "      best_model=epoch+1\n",
        "    print(f\"Best model is: {best_model}\")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "execution_time = int(execution_time/60)\n",
        "print(\"Execution time:\", execution_time, \"minutes\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}